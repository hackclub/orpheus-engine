{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from math import log1p, ceil\n",
    "import datetime\n",
    "import io # For reading string CSVs\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import polars as pl\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer # For handling any NaNs before scaling\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import sqlalchemy as sa\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_tbl_rows(30) # Show more rows\n",
    "pl.Config.set_tbl_cols(80) # Show more columns by default\n",
    "\n",
    "# --- Database Connection and Query (from your code) ---\n",
    "load_dotenv()\n",
    "engine = sa.create_engine(os.environ[\"WAREHOUSE_COOLIFY_URL\"], pool_pre_ping=True)\n",
    "\n",
    "import time\n",
    "\n",
    "DAYS_BACK = 28\n",
    "# seconds‑since‑epoch threshold in Python (no DB date math)\n",
    "cutoff = int(time.time() - DAYS_BACK * 86_400)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    /* convert only the rows that survive the WHERE filter */\n",
    "    CASE\n",
    "        WHEN time > 32503680000*1000 THEN time/1000000               -- μs → s\n",
    "        WHEN time > 32503680000      THEN time/1000                  -- ms → s\n",
    "        ELSE                           time                          -- s\n",
    "    END::double precision                AS ts_sec,\n",
    "\n",
    "    *\n",
    "FROM hackatime.heartbeats\n",
    "/* pre‑filter in native units so the index on `time` is usable */\n",
    "WHERE (\n",
    "        (time <= 32503680000                  AND time >= {cutoff})           -- seconds\n",
    "     OR (time >  32503680000  AND time <= 32503680000*1000\n",
    "                                         AND time >= {cutoff*1000})          -- milliseconds\n",
    "     OR (time >  32503680000*1000          AND time >= {cutoff*1000000})      -- microseconds\n",
    ")\n",
    "AND category = 'coding'\n",
    "AND entity NOT IN ('test.txt', 'welcome.txt')\n",
    "ORDER BY heartbeats.user_id, heartbeats.time;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.begin() as conn:\n",
    "        heartbeats = pl.read_database(query, connection=conn, infer_schema_length=None)\n",
    "    # already ordered correctly by query, but keep for safety\n",
    "    heartbeats = heartbeats.sort([\"user_id\", \"ts_sec\"])\n",
    "    print(f\"Successfully loaded {heartbeats.shape[0]} heartbeats from the database.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load data from database: {e}\")\n",
    "    print(\"Proceeding with empty or sample data (if defined below).\")\n",
    "\n",
    "heartbeats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known good/bad user IDs for validation (replace with your actual lists)\n",
    "KNOWN_GOOD_USER_IDS = {\n",
    "    1, # max\n",
    "    2, # zrl\n",
    "    104, # acon\n",
    "    69, # malted\n",
    "    864, # thomas\n",
    "    664, # lux\n",
    "    10, # annabel\n",
    "    41, # neon\n",
    "}\n",
    "KNOWN_BAD_USER_IDS = {\n",
    "    1613,\n",
    "    1728,\n",
    "    18,\n",
    "    1688\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous part of Step 1 remains the same...\n",
    "\n",
    "if heartbeats is not None and not heartbeats.is_empty():\n",
    "    # Basic type conversions and fill nulls\n",
    "    numerical_cols_to_fill_zero = [\"line_additions\", \"line_deletions\", \"lineno\", \"lines\", \"cursorpos\", \"project_root_count\"]\n",
    "    fill_zero_exprs = [pl.col(c).fill_null(0) for c in numerical_cols_to_fill_zero if c in heartbeats.columns]\n",
    "\n",
    "    categorical_cols_to_fill_unknown = [\"branch\", \"language\", \"machine\", \"project\", \"editor\"] # `editor` might be from user_agent\n",
    "    fill_unknown_exprs = [pl.col(c).fill_null(\"Unknown\") for c in categorical_cols_to_fill_unknown if c in heartbeats.columns]\n",
    "\n",
    "    heartbeats_processed = heartbeats.with_columns(\n",
    "        fill_zero_exprs +\n",
    "        fill_unknown_exprs +\n",
    "        [\n",
    "            pl.col(\"is_write\").cast(pl.Boolean).fill_null(False),\n",
    "            pl.col(\"ts_sec\").cast(pl.Float64),\n",
    "            pl.from_epoch(pl.col(\"ts_sec\").round(0).cast(pl.Int64), time_unit=\"s\").alias(\"datetime_utc\"), # Rounded for from_epoch\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # User Agent Parsing (Simplified example - you might need a more robust parser)\n",
    "    # This extracts a potential plugin/editor name like 'vscode-wakatime', 'cursor', etc.\n",
    "    def extract_plugin_name_simple(user_agent_str: str | None) -> str: # Added type hint for input\n",
    "        if user_agent_str is None: return \"Unknown\"\n",
    "        # Example: wakatime/v1.115.2 (darwin-23.3.0-arm64) go1.24.2 cursor/1.96.2 vscode-wakatime/25.0.1\n",
    "        # Look for patterns like editor/version or plugin-wakatime/version\n",
    "        # This regex tries to find something like 'vscode-wakatime' or 'cursor'\n",
    "        \n",
    "        # Try to find specific known patterns first\n",
    "        patterns = [\n",
    "            r\"vscode-wakatime(?:/[\\d\\.]+)?\", # vscode-wakatime/25.0.1 or vscode-wakatime\n",
    "            r\"cursor(?:/[\\d\\.]+)?\",          # cursor/1.96.2 or cursor\n",
    "            r\"jetbrains(?:-\\w+)?(?:/[\\d\\.]+)?\", # jetbrains-phpstorm/2023.1 or jetbrains/2023.1\n",
    "            r\"wakatime-cli(?:/[\\w\\.\\-]+)?\",  # wakatime-cli/0.0.0-dev.9+git.732ce039\n",
    "            r\"vim(?:/[\\d\\.]+)?\",\n",
    "            r\"neovim(?:/[\\d\\.]+)?\",\n",
    "            r\"sublime(?:/[\\d\\.]+)?\",\n",
    "            r\"atom(?:/[\\d\\.]+)?\",\n",
    "            r\"jupyter(?:/[\\d\\.]+)?\",\n",
    "            r\"wakatime(?:/[\\w\\.\\-]+)?\" # General wakatime prefix\n",
    "        ]\n",
    "        \n",
    "        for p_str in patterns:\n",
    "            match = re.search(p_str, user_agent_str, re.IGNORECASE)\n",
    "            if match:\n",
    "                name = match.group(0).lower()\n",
    "                if 'vscode-wakatime' in name or 'vscode' in name: return 'vscode'\n",
    "                if 'cursor' in name: return 'cursor'\n",
    "                if 'jetbrains' in name: return 'jetbrains'\n",
    "                if 'neovim' in name: return 'neovim'\n",
    "                if 'vim' in name: return 'vim'\n",
    "                if 'sublime' in name: return 'sublime'\n",
    "                if 'atom' in name: return 'atom'\n",
    "                if 'jupyter' in name: return 'jupyter'\n",
    "                if 'wakatime-cli' in name: return 'wakatime-cli'\n",
    "                if 'wakatime' in name: return 'wakatime-generic' # a fallback if no specific editor named\n",
    "                return name.split('/')[0].split('-')[0] # Basic cleaning\n",
    "\n",
    "        # Fallback to the first part if it looks like an agent name\n",
    "        first_part_match = re.match(r\"([a-zA-Z0-9_-]+)/\", user_agent_str)\n",
    "        if first_part_match:\n",
    "            return first_part_match.group(1)\n",
    "        return \"Unknown\"\n",
    "\n",
    "    heartbeats_processed = heartbeats_processed.with_columns(\n",
    "        # CORRECTED LINE:\n",
    "        pl.col(\"user_agent\").map_elements(extract_plugin_name_simple, return_dtype=pl.Utf8).alias(\"plugin_name\")\n",
    "    )\n",
    "    \n",
    "    print(\"Data cleaning and preparation complete.\")\n",
    "    print(heartbeats_processed.head())\n",
    "    print(\"\\nValue counts for `plugin_name`:\")\n",
    "    print(heartbeats_processed['plugin_name'].value_counts())\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Step 1 as heartbeats data is not loaded.\")\n",
    "    heartbeats_processed = pl.DataFrame() # Empty dataframe to prevent errors later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEARTBEAT_TIMEOUT = 120.0  # seconds\n",
    "\n",
    "if not heartbeats_processed.is_empty():\n",
    "    # Calculate time diff to next heartbeat within each user's activity\n",
    "    # Ensure sorting by user_id and ts_sec first\n",
    "    heartbeats_processed = heartbeats_processed.sort([\"user_id\", \"ts_sec\"])\n",
    "    \n",
    "    heartbeats_timed = heartbeats_processed.with_columns(\n",
    "        (pl.col(\"ts_sec\").shift(-1).over(\"user_id\") - pl.col(\"ts_sec\")).alias(\"time_to_next_hb\")\n",
    "    )\n",
    "\n",
    "    # Calculate duration for each heartbeat\n",
    "    heartbeats_timed = heartbeats_timed.with_columns(\n",
    "        pl.when(pl.col(\"time_to_next_hb\").is_null())\n",
    "        .then(pl.lit(HEARTBEAT_TIMEOUT))\n",
    "        .when(pl.col(\"time_to_next_hb\") > HEARTBEAT_TIMEOUT)\n",
    "        .then(pl.lit(HEARTBEAT_TIMEOUT))\n",
    "        .otherwise(pl.col(\"time_to_next_hb\"))\n",
    "        .alias(\"heartbeat_coded_duration_sec\")\n",
    "    )\n",
    "\n",
    "    # Define sessions\n",
    "    heartbeats_sessioned = heartbeats_timed.with_columns(\n",
    "        (pl.col(\"ts_sec\") - pl.col(\"ts_sec\").shift(1).over(\"user_id\")).alias(\"time_from_prev_hb\")\n",
    "    )\n",
    "\n",
    "    heartbeats_sessioned = heartbeats_sessioned.with_columns(\n",
    "        pl.when(pl.col(\"time_from_prev_hb\").is_null() | (pl.col(\"time_from_prev_hb\") > HEARTBEAT_TIMEOUT))\n",
    "        .then(True)\n",
    "        .otherwise(False)\n",
    "        .alias(\"is_session_start\")\n",
    "    )\n",
    "\n",
    "    heartbeats_sessioned = heartbeats_sessioned.with_columns(\n",
    "        pl.col(\"is_session_start\").cum_sum().over(\"user_id\").alias(\"session_id_within_user\")\n",
    "    )\n",
    "    heartbeats_sessioned = heartbeats_sessioned.with_columns(\n",
    "        (pl.col(\"user_id\").cast(pl.Utf8) + \"_s\" + pl.col(\"session_id_within_user\").cast(pl.Utf8)).alias(\"global_session_id\")\n",
    "    )\n",
    "    \n",
    "    print(\"Coding durations and sessions calculated.\")\n",
    "    print(heartbeats_sessioned.select([\n",
    "        \"user_id\", \"ts_sec\", \"time_to_next_hb\", \"heartbeat_coded_duration_sec\", \n",
    "        \"time_from_prev_hb\", \"is_session_start\", \"global_session_id\"\n",
    "    ]).head(10))\n",
    "else:\n",
    "    print(\"Skipping Step 2 as heartbeats_processed is empty.\")\n",
    "    heartbeats_sessioned = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not heartbeats_sessioned.is_empty():\n",
    "    user_features_list = []\n",
    "\n",
    "    # --- 1. Total Coding Time & Basic Counts ---\n",
    "    user_total_time_and_counts = heartbeats_sessioned.group_by(\"user_id\").agg(\n",
    "        pl.sum(\"heartbeat_coded_duration_sec\").alias(\"total_coded_seconds\"),\n",
    "        pl.len().alias(\"total_heartbeats\"), # CORRECTED: pl.count() -> pl.len() for row count in group_by\n",
    "        # CORRECTED: Chain .sum() onto the expression\n",
    "        pl.col(\"is_write\").cast(pl.UInt8).sum().alias(\"num_write_heartbeats\") \n",
    "    ).with_columns(\n",
    "        (pl.col(\"num_write_heartbeats\") / pl.col(\"total_heartbeats\")).fill_null(0).alias(\"ratio_write_heartbeats\"),\n",
    "        (pl.col(\"total_heartbeats\") / (pl.col(\"total_coded_seconds\") / 3600.0 + 1e-6)) # Add epsilon to avoid div by zero\n",
    "            .fill_null(0).alias(\"heartbeats_per_coding_hour\")\n",
    "    )\n",
    "    user_features_list.append(user_total_time_and_counts.drop(\"num_write_heartbeats\")) # Drop intermediate\n",
    "\n",
    "    # --- 2. Session-based features ---\n",
    "    # (Rest of the code for session aggregates)\n",
    "    session_aggregates = heartbeats_sessioned.group_by(\"user_id\", \"global_session_id\").agg(\n",
    "        pl.sum(\"heartbeat_coded_duration_sec\").alias(\"session_duration_sec\"),\n",
    "        pl.len().alias(\"num_heartbeats_in_session\"), # CORRECTED: pl.count() -> pl.len()\n",
    "        pl.min(\"ts_sec\").alias(\"session_start_ts\"),\n",
    "        pl.max(\"ts_sec\").alias(\"session_end_ts\")\n",
    "    )\n",
    "    \n",
    "    user_session_stats = session_aggregates.group_by(\"user_id\").agg(\n",
    "        pl.len().alias(\"num_sessions\"), # If counting session_ids per user\n",
    "        pl.mean(\"session_duration_sec\").alias(\"avg_session_duration_sec\"),\n",
    "        pl.max(\"session_duration_sec\").alias(\"max_session_duration_sec\"),\n",
    "        pl.median(\"session_duration_sec\").alias(\"median_session_duration_sec\"),\n",
    "        pl.std(\"session_duration_sec\").fill_null(0).alias(\"std_session_duration_sec\"),\n",
    "        pl.mean(\"num_heartbeats_in_session\").alias(\"avg_heartbeats_per_session\"),\n",
    "        (pl.col(\"session_duration_sec\").filter(pl.col(\"session_duration_sec\") > 4 * 3600).count()).alias(\"num_long_sessions_gt_4h\") # Alternative way to count conditional\n",
    "        # Or using when/then/otherwise/sum for conditional count:\n",
    "        # (pl.when(pl.col(\"session_duration_sec\") > 4 * 3600).then(1).otherwise(0)).sum().alias(\"num_long_sessions_gt_4h\")\n",
    "    )\n",
    "    user_features_list.append(user_session_stats)\n",
    "\n",
    "    # --- 3. Activity Regularity ---\n",
    "    user_activity_regularity = heartbeats_sessioned.group_by(\"user_id\").agg(\n",
    "        pl.mean(\"time_from_prev_hb\").fill_null(HEARTBEAT_TIMEOUT).alias(\"avg_time_between_heartbeats\"), # Fill null for first hb\n",
    "        pl.std(\"time_from_prev_hb\").fill_null(0).alias(\"std_time_between_heartbeats\")\n",
    "    )\n",
    "    user_features_list.append(user_activity_regularity)\n",
    "\n",
    "    # --- 4. File Interaction / \"No discernible changes\" ---\n",
    "    heartbeats_changes = heartbeats_sessioned.with_columns(\n",
    "        ((pl.col(\"entity\") == pl.col(\"entity\").shift(1).over(\"user_id\")) & \\\n",
    "         (pl.col(\"fields_hash\") == pl.col(\"fields_hash\").shift(1).over(\"user_id\")) & \\\n",
    "         (pl.col(\"time_from_prev_hb\") <= HEARTBEAT_TIMEOUT) \n",
    "        ).alias(\"is_no_change_streak_hb\")\n",
    "    )\n",
    "    \n",
    "    user_no_change_stats = heartbeats_changes.group_by(\"user_id\").agg(\n",
    "        # CORRECTED: Chain .sum()\n",
    "        pl.col(\"is_no_change_streak_hb\").cast(pl.UInt8).sum().alias(\"num_no_change_streak_heartbeats\")\n",
    "    ).join(user_total_time_and_counts.select(\"user_id\", \"total_heartbeats\"), on=\"user_id\", how=\"left\")\n",
    "    \n",
    "    user_no_change_stats = user_no_change_stats.with_columns(\n",
    "        (pl.col(\"num_no_change_streak_heartbeats\") / pl.col(\"total_heartbeats\")).fill_null(0).alias(\"ratio_no_change_streak_hbs\")\n",
    "    )\n",
    "    user_features_list.append(user_no_change_stats.select(\"user_id\", \"ratio_no_change_streak_hbs\"))\n",
    "\n",
    "    # Lines changed stats\n",
    "    user_loc_stats = heartbeats_processed.group_by(\"user_id\").agg(\n",
    "        pl.sum(\"line_additions\").alias(\"total_line_additions\"),\n",
    "        pl.sum(\"line_deletions\").alias(\"total_line_deletions\"),\n",
    "    ).with_columns(\n",
    "        (pl.col(\"total_line_additions\") + pl.col(\"total_line_deletions\")).alias(\"total_loc_changed\")\n",
    "    ).join(user_total_time_and_counts.select(\"user_id\", \"total_coded_seconds\"), on=\"user_id\", how=\"left\")\n",
    "    \n",
    "    user_loc_stats = user_loc_stats.with_columns(\n",
    "        (pl.col(\"total_loc_changed\") / (pl.col(\"total_coded_seconds\") / 3600.0 + 1e-6)) \n",
    "            .fill_null(0).alias(\"loc_changed_per_hour\")\n",
    "    )\n",
    "    user_features_list.append(user_loc_stats.select([\"user_id\", \"total_loc_changed\", \"loc_changed_per_hour\"]))\n",
    "\n",
    "    # --- 5. Diversity of Activity ---\n",
    "    user_diversity_stats = heartbeats_processed.group_by(\"user_id\").agg(\n",
    "        pl.n_unique(\"project\").alias(\"num_unique_projects\"),\n",
    "        pl.n_unique(\"language\").alias(\"num_unique_languages\"),\n",
    "        pl.n_unique(\"entity\").alias(\"num_unique_files\"),\n",
    "        pl.n_unique(\"ip_address\").alias(\"num_unique_ips\"),\n",
    "        pl.n_unique(\"operating_system\").alias(\"num_unique_os\"),\n",
    "        pl.n_unique(\"plugin_name\").alias(\"num_unique_plugins\")\n",
    "    )\n",
    "    user_features_list.append(user_diversity_stats)\n",
    "\n",
    "    # --- 6. Coding Time Patterns ---\n",
    "    heartbeats_with_hour = heartbeats_processed.with_columns(\n",
    "        pl.col(\"datetime_utc\").dt.hour().alias(\"hour_of_day_utc\")\n",
    "    )\n",
    "    user_time_patterns = heartbeats_with_hour.group_by(\"user_id\").agg(\n",
    "        # CORRECTED: Chain .sum() for conditional sums\n",
    "        (pl.when((pl.col(\"hour_of_day_utc\") >= 0) & (pl.col(\"hour_of_day_utc\") < 6)).then(1).otherwise(0)).sum()\n",
    "            .truediv(pl.len()) # Use truediv for float division\n",
    "            .fill_null(0).alias(\"prop_night_coding_utc0_6\"),\n",
    "        (pl.when(pl.col(\"datetime_utc\").dt.weekday().is_in([6,7])).then(1).otherwise(0)).sum()\n",
    "            .truediv(pl.len())\n",
    "            .fill_null(0).alias(\"prop_weekend_coding\")\n",
    "    )\n",
    "    user_features_list.append(user_time_patterns)\n",
    "\n",
    "    # --- 7. Data Fullness/Consistency (Simple version) ---\n",
    "    user_data_fullness = heartbeats.group_by(\"user_id\").agg( \n",
    "        # CORRECTED: Chain .sum() for conditional sums\n",
    "        pl.col(\"language\").is_null().cast(pl.UInt8).sum().truediv(pl.len()).alias(\"prop_null_language_orig\"),\n",
    "        pl.col(\"line_additions\").is_null().cast(pl.UInt8).sum().truediv(pl.len()).alias(\"prop_null_line_additions_orig\"),\n",
    "        (pl.col(\"branch\").is_null() | (pl.col(\"branch\") == \"\")).cast(pl.UInt8).sum().truediv(pl.len()).alias(\"prop_empty_branch_orig\")\n",
    "    )\n",
    "    user_features_list.append(user_data_fullness)\n",
    "    \n",
    "    # --- Combine all feature DataFrames ---\n",
    "    if user_features_list:\n",
    "        df_features = user_features_list[0]\n",
    "        for i in range(1, len(user_features_list)):\n",
    "            df_features = df_features.join(user_features_list[i], on=\"user_id\", how=\"left\")\n",
    "\n",
    "        df_features = df_features.fill_null(0) # Global fillna after all joins\n",
    "        \n",
    "        cols_to_log_transform = [\"total_coded_seconds\", \"total_heartbeats\", \"total_loc_changed\", \"max_session_duration_sec\"]\n",
    "        for col_name in cols_to_log_transform:\n",
    "            if col_name in df_features.columns:\n",
    "                 df_features = df_features.with_columns(\n",
    "                    pl.col(col_name).log1p().alias(f\"{col_name}_log1p\")\n",
    "                 )\n",
    "        \n",
    "        print(\"Feature engineering complete.\")\n",
    "        print(f\"Generated {len(df_features.columns) -1} features for {df_features.shape[0]} users.\")\n",
    "        print(df_features.head())\n",
    "    else:\n",
    "        print(\"No features generated, list is empty.\")\n",
    "        df_features = pl.DataFrame()\n",
    "else:\n",
    "    print(\"Skipping Step 3 as heartbeats_sessioned is empty.\")\n",
    "    df_features = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_features.is_empty() and \"user_id\" in df_features.columns:\n",
    "    # Prepare data for sklearn\n",
    "    user_ids_for_results = df_features[\"user_id\"].to_list() # Keep user_ids for mapping results\n",
    "    \n",
    "    # Select only feature columns (all numeric, including log-transformed ones if created)\n",
    "    # Exclude original versions of log-transformed columns if you prefer\n",
    "    feature_columns = [\n",
    "        col for col in df_features.columns \n",
    "        if col not in [\"user_id\"] and (\"_log1p\" in col or col not in [c.replace(\"_log1p\",\"\") for c in df_features.columns if \"_log1p\" in c])\n",
    "    ]\n",
    "    # Ensure no non-numeric columns slip through\n",
    "    X_df = df_features.select(feature_columns)\n",
    "    final_feature_columns = []\n",
    "    for col_name in X_df.columns:\n",
    "        if X_df[col_name].dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64]:\n",
    "            final_feature_columns.append(col_name)\n",
    "        else:\n",
    "            print(f\"Warning: Column {col_name} is not numeric ({X_df[col_name].dtype}) and will be excluded from model training.\")\n",
    "            \n",
    "    X = X_df.select(final_feature_columns).to_numpy()\n",
    "    print(f\"Training model with {X.shape[1]} features: {final_feature_columns}\")\n",
    "\n",
    "    # Sklearn pipeline\n",
    "    # `contamination` is the expected proportion of outliers. 'auto' is often a good start.\n",
    "    # You can tune this based on how many anomalies you expect or can investigate.\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')), # Handles any lingering NaNs\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('iso_forest', IsolationForest(n_estimators=100,       # Default, can tune\n",
    "                                       contamination='auto', # Expected proportion of outliers\n",
    "                                       # contamination=0.05, # Or set a specific value e.g. 5%\n",
    "                                       random_state=42,\n",
    "                                       n_jobs=-1))          # Use all available cores\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X)\n",
    "\n",
    "    # Get anomaly scores and predictions\n",
    "    # decision_function: The lower, the more abnormal. Outliers have lower scores.\n",
    "    # For IF, scores are not bounded, but typically negative for outliers.\n",
    "    anomaly_scores = pipeline.decision_function(X) # No need to re-transform X, pipeline handles it\n",
    "    \n",
    "    # predict: -1 for outliers (anomalies), 1 for inliers (normal)\n",
    "    predictions = pipeline.predict(X)\n",
    "\n",
    "    # Add results back to df_features for analysis\n",
    "    df_results = df_features.with_columns([\n",
    "        pl.Series(\"anomaly_score\", anomaly_scores),\n",
    "        pl.Series(\"prediction\", predictions) # -1 for anomaly, 1 for normal\n",
    "    ])\n",
    "    \n",
    "    print(\"Model training and prediction complete.\")\n",
    "    print(df_results.select([\"user_id\", \"anomaly_score\", \"prediction\"]).sort(\"anomaly_score\").head())\n",
    "    print(f\"Number of users flagged as anomalies (prediction = -1): {df_results.filter(pl.col('prediction') == -1).shape[0]}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Step 4 as df_features is empty or missing 'user_id'.\")\n",
    "    df_results = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results.is_empty() and \"user_id\" in df_results.columns:\n",
    "    # Add ground_truth_label for evaluation\n",
    "    df_results = df_results.with_columns(\n",
    "        pl.when(pl.col(\"user_id\").is_in(KNOWN_GOOD_USER_IDS)).then(0) # 0 for good\n",
    "        .when(pl.col(\"user_id\").is_in(KNOWN_BAD_USER_IDS)).then(1)   # 1 for bad\n",
    "        .otherwise(None) # None for users not in known lists\n",
    "        .alias(\"ground_truth_label\")\n",
    "    )\n",
    "\n",
    "    df_results_sorted = df_results.sort(\"anomaly_score\")\n",
    "\n",
    "    print(\"\\n--- Top 10 Most Anomalous Users ---\")\n",
    "    print(df_results_sorted.select([\"user_id\", \"anomaly_score\", \"prediction\", \"ground_truth_label\", \"total_coded_seconds_log1p\", \"heartbeats_per_coding_hour\", \"ratio_no_change_streak_hbs\"]).head(10))\n",
    "\n",
    "    known_bad_flagged = df_results_sorted.filter((pl.col(\"ground_truth_label\") == 1))\n",
    "    print(\"\\n--- Scores for Known BAD Users ---\")\n",
    "    if not known_bad_flagged.is_empty():\n",
    "        print(known_bad_flagged.select([\"user_id\", \"anomaly_score\", \"prediction\", \"ground_truth_label\"]))\n",
    "        print(f\"Total known bad: {len(KNOWN_BAD_USER_IDS)}. Found by model: {known_bad_flagged.filter(pl.col('prediction') == -1).shape[0]}\")\n",
    "    else:\n",
    "        print(\"No known bad users in the dataset or results.\")\n",
    "        \n",
    "    false_positives = df_results_sorted.filter((pl.col(\"ground_truth_label\") == 0) & (pl.col(\"prediction\") == -1))\n",
    "    print(\"\\n--- Known GOOD Users Flagged as Anomalies (False Positives) ---\")\n",
    "    if not false_positives.is_empty():\n",
    "        print(false_positives.select([\"user_id\", \"anomaly_score\", \"prediction\", \"ground_truth_label\"]))\n",
    "    else:\n",
    "        print(\"No known good users were flagged as anomalies.\")\n",
    "\n",
    "    # --- Basic Metrics (if enough known labels) ---\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "    # Filter for users with known ground truth\n",
    "    df_known_users = df_results.filter(pl.col(\"ground_truth_label\").is_not_null())\n",
    "\n",
    "    if not df_known_users.is_empty() and df_known_users[\"ground_truth_label\"].n_unique() > 1:\n",
    "        y_true_known = df_known_users[\"ground_truth_label\"].to_numpy()\n",
    "        # Map IF predictions: 1 (inlier) -> 0 (normal class for metrics), -1 (outlier) -> 1 (anomaly class for metrics)\n",
    "        y_pred_known_if_mapped = np.array([1 if p == -1 else 0 for p in df_known_users[\"prediction\"].to_numpy()])\n",
    "        \n",
    "        # decision_function scores: lower means more anomalous. For ROC AUC, higher scores usually mean positive class.\n",
    "        # So, we use -anomaly_scores as the score for the \"positive\" (anomalous) class.\n",
    "        y_scores_known_if = -df_known_users[\"anomaly_score\"].to_numpy()\n",
    "\n",
    "        print(\"\\n--- Evaluation Metrics on Known Users ---\")\n",
    "        print(f\"Number of known good users: {sum(y_true_known == 0)}\")\n",
    "        print(f\"Number of known bad users: {sum(y_true_known == 1)}\")\n",
    "\n",
    "        print(\"\\nClassification Report (IF predictions vs Ground Truth):\")\n",
    "        # target_names=['Good User (Inlier)', 'Bad User (Outlier)']\n",
    "        print(classification_report(y_true_known, y_pred_known_if_mapped, zero_division=0))\n",
    "\n",
    "        print(\"\\nConfusion Matrix ([Good, Bad] x [Predicted Good, Predicted Bad]):\")\n",
    "        # Rows: True class (Good, Bad). Cols: Predicted class (Good, Bad)\n",
    "        # TN  FP\n",
    "        # FN  TP\n",
    "        cm = confusion_matrix(y_true_known, y_pred_known_if_mapped)\n",
    "        print(cm)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true_known, y_scores_known_if)\n",
    "            print(f\"\\nROC AUC Score (on known users): {roc_auc:.4f}\")\n",
    "            \n",
    "            precision, recall, _ = precision_recall_curve(y_true_known, y_scores_known_if)\n",
    "            pr_auc = auc(recall, precision) # Area under PR curve\n",
    "            print(f\"Precision-Recall AUC Score (on known users): {pr_auc:.4f}\")\n",
    "\n",
    "        except ValueError as e_auc:\n",
    "            print(f\"Could not calculate AUC scores (likely only one class present in y_true after filtering): {e_auc}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\nNot enough known users with diverse labels to calculate detailed metrics.\")\n",
    "\n",
    "    # --- Further Analysis & Iteration Ideas ---\n",
    "    # 1. Inspect Features of Anomalies: Look at the feature values for users with low anomaly_scores.\n",
    "    #    Do they align with your definition of suspicious behavior?\n",
    "    #    `df_results_sorted.head(20)` is a good place to start.\n",
    "    #\n",
    "    # 2. Tune `contamination`: If IsolationForest flags too many or too few users, adjust this parameter.\n",
    "    #    It's the expected proportion of outliers.\n",
    "    #\n",
    "    # 3. Feature Refinement:\n",
    "    #    - Are any features too noisy or not discriminating well?\n",
    "    #    - Can you create more sophisticated features? E.g.,\n",
    "    #      - Longest continuous session on a single file with no `fields_hash` changes.\n",
    "    #      - Deviations from typical behavior for a given `plugin_name` (e.g., if a VSCode user suddenly has no line changes reported for hours).\n",
    "    #      - Velocity metrics: LOC changed per *actual* minute of file interaction, not just overall session time.\n",
    "    #\n",
    "    # 4. Daily Aggregations: For \"disproportionate hours day after day\", you might need to aggregate features per user per day,\n",
    "    #    then look for users with many consecutive anomalous days, or high average daily anomaly scores.\n",
    "    #    This would be a second layer of analysis on top of daily anomaly scores.\n",
    "    #\n",
    "    # 5. Model Exploration: If Isolation Forest doesn't give satisfactory results after tuning,\n",
    "    #    you could explore other unsupervised methods like Local Outlier Factor (LOF) or One-Class SVM,\n",
    "    #    though Isolation Forest is generally a strong baseline.\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Step 5 as df_results is empty or missing 'user_id'.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
