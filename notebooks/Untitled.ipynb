{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0391e7-fb4b-444c-8ffc-69285f0aeaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, warnings, joblib\n",
    "from pathlib import Path\n",
    "from math import log1p\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import polars as pl\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sqlalchemy as sa\n",
    "import tldextract, rapidjson as rj\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The 'registered_domain' property is deprecated\",\n",
    "    category=DeprecationWarning,\n",
    "    module=tldextract.__name__,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "engine = sa.create_engine(os.environ[\"WAREHOUSE_COOLIFY_URL\"], pool_pre_ping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957636b-5fc9-45d2-801b-299c12fceb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import polars as pl\n",
    "\n",
    "DAYS_BACK = 1\n",
    "# secondsâ€‘sinceâ€‘epoch threshold in Python (no DB date math)\n",
    "cutoff = int(time.time() - DAYS_BACK * 86_400)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    /* convert only the rows that survive the WHERE filter */\n",
    "    CASE\n",
    "        WHEN time > 32503680000*1000 THEN time/1000000               -- Î¼s â†’ s\n",
    "        WHEN time > 32503680000      THEN time/1000                  -- ms â†’ s\n",
    "        ELSE                           time                          -- s\n",
    "    END::double precision                AS ts_sec,\n",
    "\n",
    "    *\n",
    "FROM hackatime.heartbeats\n",
    "/* preâ€‘filter in native units so the index on `time` is usable */\n",
    "WHERE (\n",
    "        (time <= 32503680000                  AND time >= {cutoff})           -- seconds\n",
    "     OR (time >  32503680000  AND time <= 32503680000*1000\n",
    "                                         AND time >= {cutoff*1000})          -- milliseconds\n",
    "     OR (time >  32503680000*1000          AND time >= {cutoff*1000000})      -- microseconds\n",
    ")\n",
    "AND category = 'coding'\n",
    "AND entity NOT IN ('test.txt', 'welcome.txt')\n",
    "ORDER BY heartbeats.user_id, heartbeats.time;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    heartbeats = pl.read_database(query, connection=conn, infer_schema_length=None)\n",
    "\n",
    "# already ordered correctly, but keep for safety\n",
    "heartbeats = heartbeats.sort([\"user_id\", \"ts_sec\"])\n",
    "\n",
    "heartbeats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a10f5-7a02-4918-8266-1457483b7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = tldextract.TLDExtract(cache_dir=False)\n",
    "\n",
    "def dep_len(j: str) -> int:\n",
    "    try: return len(rj.loads(j)) if j else 0\n",
    "    except Exception: return 0\n",
    "\n",
    "def ua_dom(ua: str) -> str:\n",
    "    if not ua: return \"\"\n",
    "    m = re.search(r\"https?://([^ /]+)\", ua)\n",
    "    host = m.group(1) if m else ua.split()[-1]\n",
    "    td = ext(host)\n",
    "    return td.top_domain_under_public_suffix or host\n",
    "\n",
    "hb = (\n",
    "    heartbeats\n",
    "    # numeric safeâ€‘casts\n",
    "    .with_columns(\n",
    "        [pl.col(c).fill_null(0).cast(pl.Int32) for c in [\n",
    "            \"line_additions\",\"line_deletions\",\"lineno\",\"lines\",\n",
    "            \"cursorpos\",\"project_root_count\",\"source_type\"\n",
    "        ]]\n",
    "    )\n",
    "    # json / UA parsing\n",
    "    .with_columns([\n",
    "        pl.col(\"dependencies\").map_elements(dep_len, return_dtype=pl.Int32).alias(\"dep_count\"),\n",
    "        pl.col(\"user_agent\").map_elements(ua_dom, return_dtype=pl.String).alias(\"ua_domain\"),\n",
    "    ])\n",
    "    # logâ€‘scaled big counts\n",
    "    .with_columns([\n",
    "        pl.col(\"lines\").log1p().cast(pl.Float32).alias(\"log_lines\"),\n",
    "        pl.col(\"cursorpos\").log1p().cast(pl.Float32).alias(\"log_cursor\"),\n",
    "    ])\n",
    "    # deltaâ€‘t\n",
    "    .with_columns(\n",
    "        (pl.col(\"ts_sec\") - pl.col(\"ts_sec\").shift(1))\n",
    "        .over(\"user_id\")\n",
    "        .alias(\"delta_t\")\n",
    "    )\n",
    "    # ğŸ†• Add per-heartbeat zero change flag\n",
    "    .with_columns(\n",
    "        ((pl.col(\"line_additions\") == 0) & (pl.col(\"line_deletions\") == 0))\n",
    "        .cast(pl.Int8)\n",
    "        .alias(\"hb_zero_change_flag\") # Renamed to avoid confusion later\n",
    "    )\n",
    "    # Fill NaN delta_t for the first heartbeat of each user\n",
    "    .with_columns(\n",
    "        pl.col(\"delta_t\").fill_null(0) # Or another sensible default like -1 or mean? 0 seems okay here.\n",
    "    )\n",
    ")\n",
    "\n",
    "hb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238dde4-7517-4185-9956-5dba12dc1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  Feature engineering\n",
    "#  â€“ session-level features  âœ  duration-weighted user-level features\n",
    "# ---------------------------------------------------------------------\n",
    "TIMEOUT_SECONDS = 120  # 2 minutes\n",
    "\n",
    "# â”€â”€ 1.  Identify the start of each session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "hb_with_flag = hb.with_columns(\n",
    "    pl.when((pl.col(\"delta_t\") > TIMEOUT_SECONDS) | (pl.col(\"delta_t\") == 0))\n",
    "      .then(1).otherwise(0)\n",
    "      .alias(\"is_session_start_flag\")\n",
    ")\n",
    "\n",
    "hb_sessions = hb_with_flag.with_columns(\n",
    "    pl.col(\"is_session_start_flag\").cum_sum().over(\"user_id\").alias(\"duration_id\")\n",
    ")\n",
    "\n",
    "# â”€â”€ 2.  File switches inside a session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "hb_sessions = (\n",
    "    hb_sessions\n",
    "    .with_columns(\n",
    "        (pl.col(\"entity\") != pl.col(\"entity\").shift(1))\n",
    "            .over([\"user_id\", \"duration_id\"])\n",
    "            .fill_null(False)\n",
    "            .cast(pl.Int8)\n",
    "            .alias(\"is_file_switch_raw\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"is_session_start_flag\") == 1)\n",
    "          .then(0)\n",
    "          .otherwise(pl.col(\"is_file_switch_raw\"))\n",
    "          .alias(\"is_file_switch\")\n",
    "    )\n",
    "    .drop(\"is_file_switch_raw\")\n",
    ")\n",
    "\n",
    "# â”€â”€ 3.  Helper columns that drive the heuristics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "hb_sessions = hb_sessions.with_columns([\n",
    "    # Does the heartbeat belong to a Git branch?\n",
    "    ((pl.col(\"branch\").fill_null(\"\")) != \"\").cast(pl.Int8).alias(\"has_branch\"),\n",
    "\n",
    "    # Depth of the file path  (Â«/Â» count + 1)\n",
    "    (pl.col(\"entity\").fill_null(\"\").str.count_matches(\"/\") + 1).alias(\"path_depth\"),\n",
    "\n",
    "    # Change-size columns (null â†’ 0)\n",
    "    pl.col(\"line_additions\").fill_null(0).alias(\"line_additions_filled\"),\n",
    "    pl.col(\"line_deletions\").fill_null(0).alias(\"line_deletions_filled\"),\n",
    "])\n",
    "\n",
    "# â”€â”€ 4.  SESSION-LEVEL aggregation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sessions = (\n",
    "    hb_sessions\n",
    "    .group_by([\"user_id\", \"duration_id\"])\n",
    "    .agg([\n",
    "        # Timing\n",
    "        (pl.col(\"ts_sec\").min() * 1_000_000).cast(pl.Datetime(\"us\")).alias(\"start_time\"),\n",
    "        (pl.col(\"ts_sec\").max() * 1_000_000).cast(pl.Datetime(\"us\")).alias(\"end_time\"),\n",
    "        (pl.col(\"ts_sec\").max() - pl.col(\"ts_sec\").min() + TIMEOUT_SECONDS).alias(\"duration_seconds\"),\n",
    "\n",
    "        # Counters\n",
    "        pl.len().alias(\"hb_count\"),\n",
    "        pl.col(\"is_write\").filter(pl.col(\"is_write\") == True).count().alias(\"write_count\"),\n",
    "        pl.col(\"is_file_switch\").sum().alias(\"file_switches\"),\n",
    "\n",
    "        # Diversity\n",
    "        pl.col(\"language\").n_unique().alias(\"language_diversity\"),\n",
    "        pl.col(\"editor\").n_unique().alias(\"editor_diversity\"),\n",
    "        pl.col(\"project\").n_unique().alias(\"project_diversity\"),\n",
    "\n",
    "        # Source-control context\n",
    "        pl.col(\"has_branch\").mean().alias(\"branch_presence_pct\"),\n",
    "\n",
    "        # Path & change-size stats\n",
    "        pl.col(\"path_depth\").mean().alias(\"avg_path_depth\"),\n",
    "        pl.col(\"line_additions_filled\").mean().alias(\"avg_lines_added\"),\n",
    "        pl.col(\"line_deletions_filled\").mean().alias(\"avg_lines_deleted\"),\n",
    "        pl.col(\"line_additions_filled\").var().alias(\"var_lines_added\"),\n",
    "        pl.col(\"line_deletions_filled\").var().alias(\"var_lines_deleted\"),\n",
    "    ])\n",
    "    # Ignore â€œmicro-sessionsâ€\n",
    "    .filter(pl.col(\"hb_count\") >= 3)\n",
    ")\n",
    "\n",
    "# â”€â”€ 5.  Derived per-session ratios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "session_features = (\n",
    "    sessions.with_columns([\n",
    "        # Average gap between heartbeats\n",
    "        (pl.col(\"duration_seconds\") / pl.col(\"hb_count\").replace(0, 1e-9))\n",
    "            .fill_nan(0).alias(\"avg_seconds_between_hb\"),\n",
    "\n",
    "        # Write-vs-read mix\n",
    "        (pl.col(\"write_count\") / pl.col(\"hb_count\").replace(0, 1e-9) * 100)\n",
    "            .fill_nan(0).alias(\"write_percentage\"),\n",
    "\n",
    "        # Context-switch entropy & save cadence\n",
    "        (pl.col(\"file_switches\") / (pl.col(\"duration_seconds\") / 60))\n",
    "            .fill_nan(0).alias(\"file_switches_per_minute\"),\n",
    "\n",
    "        (pl.col(\"hb_count\") / (pl.col(\"duration_seconds\") / 60))\n",
    "            .fill_nan(0).alias(\"saves_per_minute\"),\n",
    "    ])\n",
    "    .sort([\"user_id\", \"duration_id\"])\n",
    ")\n",
    "\n",
    "# â”€â”€ 6.  USER-LEVEL, duration-weighted aggregation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "user_agg = (\n",
    "    session_features\n",
    "    .group_by(\"user_id\")\n",
    "    .agg([\n",
    "        # Total time spent coding\n",
    "        pl.col(\"duration_seconds\").sum().alias(\"total_duration_seconds\"),\n",
    "\n",
    "        # Duration-weighted sums  (later turned into weighted averages)\n",
    "        (pl.col(\"write_percentage\") * pl.col(\"duration_seconds\")).sum().alias(\"write_pct_wt_sum\"),\n",
    "        (pl.col(\"saves_per_minute\") * pl.col(\"duration_seconds\")).sum().alias(\"saves_per_min_wt_sum\"),\n",
    "        (pl.col(\"file_switches_per_minute\") * pl.col(\"duration_seconds\")).sum().alias(\"switches_per_min_wt_sum\"),\n",
    "        (pl.col(\"branch_presence_pct\") * pl.col(\"duration_seconds\")).sum().alias(\"branch_pct_wt_sum\"),\n",
    "        (pl.col(\"language_diversity\") * pl.col(\"duration_seconds\")).sum().alias(\"lang_div_wt_sum\"),\n",
    "        (pl.col(\"avg_path_depth\") * pl.col(\"duration_seconds\")).sum().alias(\"path_depth_wt_sum\"),\n",
    "\n",
    "        # â€œWorst-caseâ€ extremes (independent of session length)\n",
    "        pl.col(\"write_percentage\").max().alias(\"max_write_percentage\"),\n",
    "        pl.col(\"saves_per_minute\").max().alias(\"max_saves_per_minute\"),\n",
    "        pl.col(\"avg_seconds_between_hb\").min().alias(\"min_avg_seconds_between_hb\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "user_features = (\n",
    "    user_agg.with_columns([\n",
    "        (pl.col(\"write_pct_wt_sum\")      / pl.col(\"total_duration_seconds\")).alias(\"write_percentage_wt\"),\n",
    "        (pl.col(\"saves_per_min_wt_sum\")  / pl.col(\"total_duration_seconds\")).alias(\"saves_per_minute_wt\"),\n",
    "        (pl.col(\"switches_per_min_wt_sum\")/ pl.col(\"total_duration_seconds\")).alias(\"file_switches_per_minute_wt\"),\n",
    "        (pl.col(\"branch_pct_wt_sum\")     / pl.col(\"total_duration_seconds\")).alias(\"branch_presence_pct_wt\"),\n",
    "        (pl.col(\"lang_div_wt_sum\")       / pl.col(\"total_duration_seconds\")).alias(\"language_diversity_wt\"),\n",
    "        (pl.col(\"path_depth_wt_sum\")     / pl.col(\"total_duration_seconds\")).alias(\"avg_path_depth_wt\"),\n",
    "    ])\n",
    "    .drop([\n",
    "        \"write_pct_wt_sum\",\"saves_per_min_wt_sum\",\"switches_per_min_wt_sum\",\n",
    "        \"branch_pct_wt_sum\",\"lang_div_wt_sum\",\"path_depth_wt_sum\",\n",
    "    ])\n",
    "    .sort(\"user_id\")\n",
    ")\n",
    "\n",
    "# â”€â”€ 7.  Quick sanity-check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nSession-level features (sample):\")\n",
    "print(session_features.filter(pl.col(\"user_id\").is_in([2, 1613])).head())\n",
    "\n",
    "print(\"\\nDuration-weighted USER-level features:\")\n",
    "user_features.filter(pl.col(\"user_id\").is_in([2, 1613]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64777d1-f82b-4ba8-b4f3-2abe9129cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NUMERIC_COLS includes the new/renamed features and excludes dropped ones\n",
    "NUMERIC_COLS = [\n",
    "    # c for c in user_features.columns if c not in (\n",
    "    #     \"user_id\", \"total_duration_seconds\"\n",
    "    # )\n",
    "    \"max_saves_per_minute\",\n",
    "    \"write_percentage_wt\",\n",
    "    \"saves_per_minute_wt\",\n",
    "    \"file_switches_per_minute_wt\",\n",
    "    \"language_diversity_wt\",\n",
    "]\n",
    "print(f\"Using {len(NUMERIC_COLS)} numeric features:\")\n",
    "print(NUMERIC_COLS)\n",
    "\n",
    "print(user_features.filter(pl.col(\"user_id\") == 2).select(NUMERIC_COLS))\n",
    "\n",
    "X = user_features.select(NUMERIC_COLS).to_numpy()\n",
    "\n",
    "# Verify shape\n",
    "print(f\"Feature matrix X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9ddac-0fe3-4ce2-8339-8ec2abd68a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD_USERS = [\n",
    "    1, # max\n",
    "    2, # zrl\n",
    "    104, # acon\n",
    "    69, # malted\n",
    "    864, # thomas\n",
    "    664, # lux\n",
    "    10, # annabel\n",
    "\n",
    "    # hack clubbers that look ok\n",
    "    1256, \n",
    "    1309,\n",
    "    1460,\n",
    "    1561,\n",
    "    40,\n",
    "    48,\n",
    "    1729,\n",
    "    1591\n",
    "]\n",
    "\n",
    "BAD_USERS = [\n",
    "    1613,\n",
    "    1728,\n",
    "    18,\n",
    "    1688\n",
    "]\n",
    "\n",
    "TRUSTED = (\n",
    "    user_features.filter(~pl.col(\"user_id\").is_in(BAD_USERS))\n",
    "            .select(NUMERIC_COLS)\n",
    "            .to_numpy()\n",
    ")\n",
    "\n",
    "print(f\"Training on {TRUSTED.shape[0]} entries\")\n",
    "\n",
    "model = Pipeline([\n",
    "    # ğŸ†• Use default StandardScaler (centers data)\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"iso\", IsolationForest(\n",
    "        n_estimators=800, contamination=0.05, # contamination on trusted set\n",
    "        bootstrap=True, random_state=42\n",
    "    ))\n",
    "]).fit(TRUSTED)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991becaf-212b-40e9-a830-bf907b14c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Assume 'features', 'model', 'X', and 'TRUSTED' are already defined\n",
    "# For example:\n",
    "# features = pl.DataFrame({...}) # Your existing features DataFrame\n",
    "# TRUSTED = np.array([...])     # Your existing TRUSTED numpy array\n",
    "# X = np.array([...])           # Your existing X numpy array\n",
    "# model = ...                   # Your trained model\n",
    "\n",
    "# bottom kâ€‘percent of trusted windows define \"too weird\"\n",
    "PCTL = 5\n",
    "# Calculate threshold based on the scores of the TRUSTED data\n",
    "scores_trusted = model.decision_function(TRUSTED)\n",
    "threshold = np.percentile(scores_trusted, PCTL)\n",
    "print(f\"cutâ€‘off at {PCTL}th percentile of trusted scores â†’ {threshold:.4f}\")\n",
    "\n",
    "# Score all data (X)\n",
    "scores = model.decision_function(X)\n",
    "\n",
    "# Add/overwrite 'anomaly_score' and then 'is_anomaly' columns\n",
    "# Using with_columns for 'anomaly_score' ensures it's overwritten if it exists.\n",
    "features = user_features.with_columns(\n",
    "    pl.Series(\"anomaly_score\", scores)  # Adds or overwrites the 'anomaly_score' column\n",
    ")\n",
    "\n",
    "print(features.head())\n",
    "# Now, use the (potentially new) 'anomaly_score' column to create/overwrite 'is_anomaly'\n",
    "features = user_features.with_columns(\n",
    "    (pl.col(\"anomaly_score\") < threshold).alias(\"is_anomaly\") # Adds or overwrites 'is_anomaly'\n",
    ")\n",
    "\n",
    "# summary\n",
    "user_stats = (\n",
    "    features.group_by(\"user_id\")\n",
    "            .agg([\n",
    "                pl.len().alias(\"windows\"),\n",
    "                pl.col(\"is_anomaly\").mean().alias(\"anomaly_rate\"),\n",
    "                pl.col(\"anomaly_score\").mean().alias(\"avg_score\"),\n",
    "                pl.col(\"anomaly_score\").min().alias(\"min_score\"),\n",
    "                pl.col(\"anomaly_score\").max().alias(\"max_score\"),\n",
    "            ])\n",
    "    .sort(\"user_id\") # Sort for consistency\n",
    ")\n",
    "print(user_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe13d15-bca3-4bc7-b0e6-3432b295c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats.filter(pl.col(\"user_id\").is_in(GOOD_USERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af82dd-872f-4906-8e4c-022f9dd2ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats.filter(pl.col(\"user_id\").is_in(BAD_USERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa52370-136d-4004-bec6-60ccaedfa6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats.filter(pl.col(\"anomaly_rate\") > 0.4, pl.col(\"windows\") > 3, ~pl.col('user_id').is_in(BAD_USERS + GOOD_USERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "# Assuming 'user_stats' DataFrame is already computed and available\n",
    "# And GOOD_USERS and BAD_USERS lists are defined from your previous cells\n",
    "\n",
    "# First, we need to get the total duration for each user\n",
    "# Assuming features DataFrame has user_id and duration_seconds columns\n",
    "user_durations = features.group_by(\"user_id\").agg(\n",
    "    pl.sum(\"duration_seconds\").alias(\"total_duration\")\n",
    ")\n",
    "\n",
    "# Join the durations with user_stats\n",
    "user_stats_with_duration = user_stats.join(user_durations, on=\"user_id\")\n",
    "\n",
    "# Convert to pandas DataFrame for easier plotting with annotations\n",
    "user_stats_pd = user_stats_with_duration.to_pandas()\n",
    "\n",
    "# Define colors for users\n",
    "def get_color(user_id):\n",
    "    if user_id in GOOD_USERS:\n",
    "        return 'green'\n",
    "    elif user_id in BAD_USERS:\n",
    "        return 'red'\n",
    "    else:\n",
    "        return 'blue' # Default color for other users\n",
    "\n",
    "user_stats_pd['color'] = user_stats_pd['user_id'].apply(get_color)\n",
    "\n",
    "plt.figure(figsize=(14, 10)) # Increased figure size a bit for clarity\n",
    "\n",
    "# Scatter plot using the 'color' column and total_duration on x-axis\n",
    "plt.scatter(user_stats_pd['total_duration'], user_stats_pd['anomaly_rate'], c=user_stats_pd['color'])\n",
    "\n",
    "plt.xlabel(\"Total Duration (seconds)\")\n",
    "plt.ylabel(\"Anomaly Rate\")\n",
    "plt.title(\"Anomaly Rate vs. Total Duration by User (Good=Green, Bad=Red)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Create a legend\n",
    "# Since scatter doesn't directly support legend items for different colors from a column in this way,\n",
    "# we create proxy artists for the legend.\n",
    "import matplotlib.lines as mlines\n",
    "green_patch = mlines.Line2D([], [], color='green', marker='o', linestyle='None', markersize=10, label='Good Users')\n",
    "red_patch = mlines.Line2D([], [], color='red', marker='o', linestyle='None', markersize=10, label='Bad Users')\n",
    "blue_patch = mlines.Line2D([], [], color='blue', marker='o', linestyle='None', markersize=10, label='Other Users')\n",
    "plt.legend(handles=[green_patch, red_patch, blue_patch])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "user_stats_pd"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
