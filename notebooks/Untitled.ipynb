{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0391e7-fb4b-444c-8ffc-69285f0aeaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, warnings, joblib\n",
    "from pathlib import Path\n",
    "from math import log1p\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import polars as pl\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sqlalchemy as sa\n",
    "import tldextract, rapidjson as rj\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The 'registered_domain' property is deprecated\",\n",
    "    category=DeprecationWarning,\n",
    "    module=tldextract.__name__,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "engine = sa.create_engine(os.environ[\"WAREHOUSE_COOLIFY_URL\"], pool_pre_ping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957636b-5fc9-45d2-801b-299c12fceb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import polars as pl\n",
    "\n",
    "DAYS_BACK = 28\n",
    "# seconds‑since‑epoch threshold in Python (no DB date math)\n",
    "cutoff = int(time.time() - DAYS_BACK * 86_400)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    user_id::int,\n",
    "    /* convert only the rows that survive the WHERE filter */\n",
    "    CASE\n",
    "        WHEN time > 32503680000*1000 THEN time/1000000               -- μs → s\n",
    "        WHEN time > 32503680000      THEN time/1000                  -- ms → s\n",
    "        ELSE                           time                          -- s\n",
    "    END::double precision                AS ts_sec,\n",
    "\n",
    "    is_write, line_additions, line_deletions,\n",
    "    branch, category, dependencies,\n",
    "    editor, language, machine, operating_system,\n",
    "    entity, project, type, user_agent,\n",
    "    lineno, lines, cursorpos, project_root_count,\n",
    "    source_type, ysws_program, ip_address\n",
    "FROM hackatime.heartbeats\n",
    "/* pre‑filter in native units so the index on `time` is usable */\n",
    "WHERE (\n",
    "        (time <= 32503680000                  AND time >= {cutoff})           -- seconds\n",
    "     OR (time >  32503680000  AND time <= 32503680000*1000\n",
    "                                         AND time >= {cutoff*1000})          -- milliseconds\n",
    "     OR (time >  32503680000*1000          AND time >= {cutoff*1000000})      -- microseconds\n",
    ")\n",
    "ORDER BY user_id, time;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    heartbeats = pl.read_database(query, connection=conn, infer_schema_length=None)\n",
    "\n",
    "# already ordered correctly, but keep for safety\n",
    "heartbeats = heartbeats.sort([\"user_id\", \"ts_sec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a10f5-7a02-4918-8266-1457483b7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = tldextract.TLDExtract(cache_dir=False)\n",
    "\n",
    "def dep_len(j: str) -> int:\n",
    "    try: return len(rj.loads(j)) if j else 0\n",
    "    except Exception: return 0\n",
    "\n",
    "def ua_dom(ua: str) -> str:\n",
    "    if not ua: return \"\"\n",
    "    m = re.search(r\"https?://([^ /]+)\", ua)\n",
    "    host = m.group(1) if m else ua.split()[-1]\n",
    "    td = ext(host)\n",
    "    return td.top_domain_under_public_suffix or host\n",
    "\n",
    "hb = (\n",
    "    heartbeats\n",
    "    # numeric safe‑casts\n",
    "    .with_columns(\n",
    "        [pl.col(c).fill_null(0).cast(pl.Int32) for c in [\n",
    "            \"line_additions\",\"line_deletions\",\"lineno\",\"lines\",\n",
    "            \"cursorpos\",\"project_root_count\",\"source_type\"\n",
    "        ]]\n",
    "    )\n",
    "    # json / UA parsing\n",
    "    .with_columns([\n",
    "        pl.col(\"dependencies\").map_elements(dep_len, return_dtype=pl.Int32).alias(\"dep_count\"),\n",
    "        pl.col(\"user_agent\").map_elements(ua_dom, return_dtype=pl.String).alias(\"ua_domain\"),\n",
    "    ])\n",
    "    # log‑scaled big counts\n",
    "    .with_columns([\n",
    "        pl.col(\"lines\").map_elements(log1p, return_dtype=pl.Float32).alias(\"log_lines\"),\n",
    "        pl.col(\"cursorpos\").map_elements(log1p, return_dtype=pl.Float32).alias(\"log_cursor\"),\n",
    "    ])\n",
    "    # delta‑t\n",
    "    .with_columns(\n",
    "        (pl.col(\"ts_sec\") - pl.col(\"ts_sec\").shift(1))\n",
    "        .over(\"user_id\")\n",
    "        .alias(\"delta_t\")\n",
    "    )\n",
    "    # 🆕 Add per-heartbeat zero change flag\n",
    "    .with_columns(\n",
    "        ((pl.col(\"line_additions\") == 0) & (pl.col(\"line_deletions\") == 0))\n",
    "        .cast(pl.Int8)\n",
    "        .alias(\"hb_zero_change_flag\") # Renamed to avoid confusion later\n",
    "    )\n",
    "    # Fill NaN delta_t for the first heartbeat of each user\n",
    "    .with_columns(\n",
    "        pl.col(\"delta_t\").fill_null(0) # Or another sensible default like -1 or mean? 0 seems okay here.\n",
    "    )\n",
    ")\n",
    "\n",
    "hb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238dde4-7517-4185-9956-5dba12dc1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 3600\n",
    "\n",
    "# ⬇️ ADD THIS STEP BACK IN ⬇️\n",
    "# Add the bucket_id column to the hb DataFrame first\n",
    "hb_with_bucket = hb.with_columns(\n",
    "    (pl.col(\"ts_sec\") // BUCKET).alias(\"bucket_id\")\n",
    ")\n",
    "# ⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️\n",
    "\n",
    "# Aggregation step (now using hb_with_bucket)\n",
    "base = (\n",
    "    # ⬇️ Use the DataFrame that now has 'bucket_id' ⬇️\n",
    "    hb_with_bucket.group_by([\"user_id\", \"bucket_id\"])\n",
    "    # ⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️\n",
    "      .agg([\n",
    "          # ── timing / volume ──────────────────────────────────────────\n",
    "          pl.len().alias(\"hb_count\"),\n",
    "          pl.col(\"delta_t\").mean().alias(\"mean_dt\"),\n",
    "          pl.col(\"delta_t\").std().fill_null(0).alias(\"std_dt\"),\n",
    "          pl.col(\"delta_t\").n_unique().alias(\"unique_dt\"),\n",
    "          # Count occurrences of the most frequent delta_t\n",
    "          pl.col(\"delta_t\").value_counts(sort=True) # Get counts per dt value\n",
    "              .struct.field(\"count\").first() # Take the count of the most frequent one\n",
    "              .fill_null(0).alias(\"dominant_dt_count\"), # Handle cases with 0/1 HB\n",
    "\n",
    "          # ── basic sums (no derived maths yet) ───────────────────────\n",
    "          pl.col(\"is_write\").sum().alias(\"write_hb_count\"), # Renamed for clarity\n",
    "          pl.col(\"line_additions\").sum().alias(\"tot_add\"),\n",
    "          pl.col(\"line_deletions\").sum().alias(\"tot_del\"),\n",
    "          pl.col(\"dep_count\").sum().alias(\"tot_dep\"),\n",
    "          # Sum of per-heartbeat zero change flags\n",
    "          pl.col(\"hb_zero_change_flag\").sum().alias(\"sum_hb_zero_change\"),\n",
    "\n",
    "          # ── diversity metrics ───────────────────────────────────────\n",
    "          pl.col(\"entity\").n_unique().alias(\"file_diversity\"),\n",
    "          pl.col(\"language\").n_unique().alias(\"lang_diversity\"),\n",
    "          pl.col(\"editor\").n_unique().alias(\"editor_switches\"),\n",
    "          pl.col(\"machine\").n_unique().alias(\"machine_switches\"),\n",
    "          pl.col(\"operating_system\").n_unique().alias(\"os_switches\"),\n",
    "          pl.col(\"ip_address\").n_unique().alias(\"ip_switches\"),\n",
    "          pl.col(\"branch\").n_unique().alias(\"branch_switches\"),\n",
    "          pl.col(\"ua_domain\").n_unique().alias(\"ua_diversity\"),\n",
    "          pl.col(\"source_type\").n_unique().alias(\"src_switches\"),\n",
    "\n",
    "          # simple sums for later ratios\n",
    "          pl.col(\"project_root_count\").sum().alias(\"sum_root_count\"),\n",
    "      ])\n",
    ")\n",
    "\n",
    "# Derived features step (uses 'base' which is derived from hb_with_bucket)\n",
    "features = (\n",
    "    base.with_columns([\n",
    "        # coefficient of variation for delta_t\n",
    "        (pl.col(\"std_dt\") / pl.col(\"mean_dt\").replace(0, 1e-9)) # Avoid division by zero\n",
    "            .fill_nan(0).fill_null(0).alias(\"dt_cv\"),\n",
    "\n",
    "        # Ratio indicating perfect uniformity in delta_t\n",
    "        (pl.col(\"unique_dt\") == 1).cast(pl.Int8).alias(\"uniform_dt_flag\"), # Renamed flag\n",
    "\n",
    "        # Average lines added/deleted per heartbeat\n",
    "        ((pl.col(\"tot_add\") + pl.col(\"tot_del\")) / pl.col(\"hb_count\"))\n",
    "            .fill_nan(0).alias(\"lines_per_hb\"),\n",
    "\n",
    "        # Flag indicating only one file touched in the window\n",
    "        (pl.col(\"file_diversity\") == 1).cast(pl.Int8).alias(\"same_file_flag\"), # Renamed flag\n",
    "\n",
    "        # Percentage of heartbeats that were write events\n",
    "        (pl.col(\"write_hb_count\") / pl.col(\"hb_count\"))\n",
    "            .fill_nan(0).alias(\"pct_write\"),\n",
    "\n",
    "        # Corrected: Ratio of heartbeats with zero line changes\n",
    "        (pl.col(\"sum_hb_zero_change\") / pl.col(\"hb_count\"))\n",
    "            .fill_nan(0).alias(\"zero_change_ratio\"),\n",
    "\n",
    "        # Average project root count per heartbeat\n",
    "        (pl.col(\"sum_root_count\") / pl.col(\"hb_count\"))\n",
    "            .fill_nan(0).alias(\"avg_root_count\"),\n",
    "\n",
    "        # Dominant delta_t frequency ratio\n",
    "        (pl.col(\"dominant_dt_count\") / pl.col(\"hb_count\"))\n",
    "             .fill_nan(0).fill_null(0).alias(\"dominant_dt_freq_ratio\")\n",
    "\n",
    "    ])\n",
    "    # Drop intermediate sum columns and potentially redundant/less informative ones\n",
    "    .drop([\n",
    "        \"write_hb_count\", \"tot_add\", \"tot_del\", \"sum_root_count\",\n",
    "        \"sum_hb_zero_change\", \"dominant_dt_count\"\n",
    "    ])\n",
    "    .sort([\"user_id\", \"bucket_id\"])\n",
    ")\n",
    "\n",
    "# Optional: Display head/tail to verify new features\n",
    "# print(features.head())\n",
    "# print(features.filter(pl.col(\"user_id\") == 1613).select([\"user_id\", \"bucket_id\", \"hb_count\", \"mean_dt\", \"std_dt\", \"dt_cv\", \"uniform_dt_flag\", \"dominant_dt_freq_ratio\", \"zero_change_ratio\", \"lines_per_hb\", \"same_file_flag\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64777d1-f82b-4ba8-b4f3-2abe9129cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NUMERIC_COLS includes the new/renamed features and excludes dropped ones\n",
    "NUMERIC_COLS = [\n",
    "    c for c in features.columns if c not in (\n",
    "        \"user_id\", \"bucket_id\"\n",
    "    )\n",
    "]\n",
    "print(f\"Using {len(NUMERIC_COLS)} numeric features:\")\n",
    "print(NUMERIC_COLS)\n",
    "\n",
    "X = features.select(NUMERIC_COLS).to_numpy()\n",
    "\n",
    "# Verify shape\n",
    "print(f\"Feature matrix X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9ddac-0fe3-4ce2-8339-8ec2abd68a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD_USERS = [\n",
    "    1, # max\n",
    "    2, # zrl\n",
    "    104, # acon\n",
    "    69, # malted\n",
    "    864, # thomas\n",
    "    664, # lux\n",
    "    10, # annabel\n",
    "\n",
    "    # hack clubbers that look ok\n",
    "    1256, \n",
    "    1309,\n",
    "    1460,\n",
    "    1561,\n",
    "    40,\n",
    "    48,\n",
    "    1729,\n",
    "    1591\n",
    "]\n",
    "\n",
    "BAD_USERS = [\n",
    "    1613,\n",
    "    1728,\n",
    "    18\n",
    "]\n",
    "\n",
    "TRUSTED = (\n",
    "    features.filter(pl.col(\"user_id\").is_in(GOOD_USERS))\n",
    "            .select(NUMERIC_COLS)\n",
    "            .to_numpy()\n",
    ")\n",
    "\n",
    "print(f\"Training on {TRUSTED.shape[0]} windows from {len(GOOD_USERS)} trusted users.\")\n",
    "\n",
    "model = Pipeline([\n",
    "    # 🆕 Use default StandardScaler (centers data)\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"iso\", IsolationForest(\n",
    "        n_estimators=800, contamination=0.01, # contamination on trusted set\n",
    "        bootstrap=True, random_state=42\n",
    "    ))\n",
    "]).fit(TRUSTED)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991becaf-212b-40e9-a830-bf907b14c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom k‑percent of trusted windows define \"too weird\"\n",
    "PCTL = 5\n",
    "# Calculate threshold based on the scores of the TRUSTED data\n",
    "scores_trusted = model.decision_function(TRUSTED)\n",
    "threshold = np.percentile(scores_trusted, PCTL)\n",
    "print(f\"cut‑off at {PCTL}th percentile of trusted scores → {threshold:.4f}\")\n",
    "\n",
    "# Score all data (X)\n",
    "scores = model.decision_function(X)\n",
    "features = (\n",
    "    features.hstack([pl.Series(\"anomaly_score\", scores)])\n",
    "            .with_columns((pl.col(\"anomaly_score\") < threshold).alias(\"is_anomaly\"))\n",
    ")\n",
    "# summary\n",
    "user_stats = (\n",
    "    features.group_by(\"user_id\")\n",
    "            .agg([\n",
    "                pl.len().alias(\"windows\"),\n",
    "                pl.col(\"is_anomaly\").mean().alias(\"anomaly_rate\"),\n",
    "                pl.col(\"anomaly_score\").mean().alias(\"avg_score\"),\n",
    "                # Add min/max score for more insight?\n",
    "                pl.col(\"anomaly_score\").min().alias(\"min_score\"),\n",
    "                pl.col(\"anomaly_score\").max().alias(\"max_score\"),\n",
    "            ])\n",
    "    .sort(\"user_id\") # Sort for consistency\n",
    ")\n",
    "print(user_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe13d15-bca3-4bc7-b0e6-3432b295c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats.filter(pl.col(\"user_id\").is_in(GOOD_USERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af82dd-872f-4906-8e4c-022f9dd2ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats.filter(pl.col(\"user_id\").is_in(BAD_USERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa52370-136d-4004-bec6-60ccaedfa6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats.filter(pl.col(\"anomaly_rate\") > 0.4, pl.col(\"windows\") > 3, ~pl.col('user_id').is_in(BAD_USERS + GOOD_USERS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
